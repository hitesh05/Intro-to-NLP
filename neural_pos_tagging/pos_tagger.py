# -*- coding: utf-8 -*-
"""Neural pos tagging.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jQ8R8mvnVizmmgQbwvPeLXv7mSzcIDkG
"""

# from google.colab import drive
# drive.mount('/content/drive')

# !pip install conllu

## imports 

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.nn.utils import clip_grad_norm_
import os
import matplotlib.pyplot as plt
import sklearn.metrics
import seaborn as sns
import random
from conllu import parse_incr

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Device available now:', device)

"""### Parsing the data"""

# !ls /content/drive/MyDrive/POS\ dataset_English

dataset = "/content/drive/MyDrive/POS dataset_English/"

train = dataset + 'en_atis-ud-train.conllu'
dev = dataset + 'en_atis-ud-dev.conllu'
test = dataset + 'en_atis-ud-test.conllu'

text = list()
text_train = list()
testers = 0
with open(train, 'r', encoding="utf-8") as f:
  for tokenlist in parse_incr(f):
    text.append(tokenlist)
    testers += 1
    text_train.append(tokenlist)
print(testers)
text_dev = list()
with open(dev, 'r', encoding="utf-8") as f:
  for tokenlist in parse_incr(f):
    text.append(tokenlist)
    testers += 1
    text_dev.append(tokenlist)
print(testers)

text_test = list()
with open(test, 'r', encoding="utf-8") as f:
  for tokenlist in parse_incr(f):
    text.append(tokenlist)
    testers += 1
    text_test.append(tokenlist)
print(testers)
print(text_train)

sent = text_train[0]
token = sent[:]
token

def make_vocab(text, pad_value):
  word_to_index = dict()
  tag_to_index = dict()
  tokens = 0
  sentences = 0
  # id, form, upos
  for ind, sentence in enumerate(text):
    sent = text[ind]
    sentences += 1
    for i, tok in enumerate(sent):
      token = sent[i]
      id = token['id']
      word = token['form']
      tag = token['upos']
      tokens += 1
      if word not in word_to_index.keys():
        word_to_index[word] = 1
      else:
        word_to_index[word]+=1
      if tag not in tag_to_index.keys():
        tag_to_index[tag] = len(tag_to_index)

  alt = dict()
  alt['<UNK>'] = 0
  for k,v in word_to_index.items():
    if v<=1:
      alt['<UNK>'] += v
    else:
      alt[k] = v

  word_to_index = alt.copy()

  l = list(word_to_index)
  word_to_idx = dict()
  for i in range(len(l)):
    word_to_idx[l[i]] = i+1
  word_to_idx['<pad>'] = PAD_VALUE
  return word_to_idx, tag_to_index, sentences, tokens

PAD_VALUE = 0
word_to_idx, tag_to_index, sentences, tokens = make_vocab(text, PAD_VALUE)
word_to_idx, tag_to_index

# ids = torch.LongTensor(tokens)
# ids_tag = torch.LongTensor(tokens)
def indexing(text, sentences, word_to_idx, tag_to_index):
  ids = list()
  ids_tag = list()
  for i in range(sentences):
    ids.append(list())
    ids_tag.append(list())
  token = 0
  for ind, sentence in enumerate(text):
    sent = text[ind]
    for i, tok in enumerate(sent):
      t = sent[i]
      word = t['form']
      tag = t['upos']
      if word not in word_to_idx.keys():
        ids[ind].append(word_to_idx['<UNK>'])
      else:
        ids[ind].append(word_to_idx[word])
      ids_tag[ind].append(tag_to_index[tag])
      token += 1
  return ids, ids_tag

ids, ids_tag = indexing(text, sentences, word_to_idx, tag_to_index)
max_len = max(len(sent) for sent in ids)
ids = np.array([np.pad(row, (PAD_VALUE, max_len-len(row))) for row in ids])
ids_tag = np.array([np.pad(row, (PAD_VALUE, max_len-len(row))) for row in ids_tag])
ids=torch.from_numpy(ids)
ids_tag=torch.from_numpy(ids_tag)
print(ids.size(), ids_tag.size())

BATCH_SIZE = 20

train_ids = ids[:4274, :]
dev_ids = ids[4274:4846, :]
test_ids = ids[4846:, :]

train_ids_tag = ids_tag[:4274]
dev_ids_tag = ids_tag[4274:4846]
test_ids_tag = ids_tag[4846:]

# num_batches = ids.size(0) // BATCH_SIZE
# ids = ids[:num_batches*BATCH_SIZE]
# ids_tag = ids_tag[:num_batches*BATCH_SIZE]

# num_batches = train_ids.size(0) // BATCH_SIZE
# train_ids = train_ids[:num_batches*BATCH_SIZE]
# train_ids_tag = train_ids_tag[:num_batches*BATCH_SIZE]

# num_batches = dev_ids.size(0) // BATCH_SIZE
# dev_ids = dev_ids[:num_batches*BATCH_SIZE]
# dev_ids_tag = dev_ids_tag[:num_batches*BATCH_SIZE]

# num_batches = test_ids.size(0) // BATCH_SIZE
# test_ids = test_ids[:num_batches*BATCH_SIZE]
# test_ids_tag = test_ids_tag[:num_batches*BATCH_SIZE]

# corpus_data = ids.view(BATCH_SIZE, -1)
# corpus_data_tag = ids_tag.view(BATCH_SIZE, -1)

# train_lines = train_ids.view(BATCH_SIZE, -1)
# train_lines_tag = train_ids_tag.view(BATCH_SIZE, -1)

# dev_lines = dev_ids.view(BATCH_SIZE, -1)
# dev_lines_tag = dev_ids_tag.view(BATCH_SIZE, -1)

# test_lines = test_ids.view(BATCH_SIZE, -1)
# test_lines_tag = test_ids_tag.view(BATCH_SIZE, -1)

# print(corpus_data.size(), train_lines.size(), dev_lines.size(), test_lines.size())

train_ids

"""### BiLSTM Model"""

class BiLSTM(nn.Module):
  def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, num_layers, dropout, pad_idx = 0):
    super(BiLSTM, self).__init__()
    self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)
    # self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True, dropout = dropout if num_layers > 1 else 0)
    self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)
    self.fc = nn.Linear(hidden_dim*2, output_dim)
    self.dropout = nn.Dropout(dropout)

  def forward(self, x):
    x = self.dropout(self.embedding(x))
    # x = self.dropout(self.embedding(x))

    out, _ = self.lstm(x)
    # out = out[:, -1, :]
    # out = out.reshape(out.size(0)*out.size(1), out.size(2))
    out = self.fc(self.dropout(out))
    tag_scores = F.log_softmax(out, dim=1)
    return out

"""### Hyperparameters"""

input_dim = len(word_to_idx.keys())
embedding_dim = 128
hidden_dim = 128
output_dim = len(tag_to_index.keys())
num_layers = 4
dropout = 0.15

batch_size = BATCH_SIZE
learning_rate = 0.01
seq_length = 30
num_epochs = 10
# num_batches = corpus_data.size(1) // seq_length

model = BiLSTM(input_dim=input_dim, embedding_dim=embedding_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers, dropout=dropout).to(device)
def init_weights(m):
    for name, param in m.named_parameters():
        nn.init.normal_(param.data, mean = 0, std = 0.1)
        
model.apply(init_weights)

model

"""### Training the model"""

criterion = nn.CrossEntropyLoss(ignore_index=PAD_VALUE)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)

# def detach(states):
#     return [state.detach() for state in states]

# def train(model,criterion,optimizer,input_data_sen, input_data_tag,batch_size,num_epochs, tag_pad_idx=0):
#   for epoch in range(num_epochs):
#     model.train()
#     epoch_loss = 0
#     epoch_acc = 0
#     # states = (torch.zeros(num_layers*2, input_data_sen.size(0), hidden_dim).to(device),
#     #           torch.zeros(num_layers*2, input_data_sen.size(0), hidden_dim).to(device))
#     for ind, i in enumerate(input_data_sen):
#       inputs = input_data_sen[ind].to(device)
#       targets = input_data_tag[ind].to(device)

#       # forward pass
#       # states = detach(states)
#       outputs = model(inputs)

#       outputs = outputs.view(-1, outputs.shape[-1])
#       targets = targets.view(-1)

#       loss = criterion(outputs, targets)

#       # backward pass
#       optimizer.zero_grad()
#       loss.backward()
#       optimizer.step()

#       epoch_loss += loss.item()
#     epoch_loss = epoch_loss / len(input_data_sen)
#     print(f'Epoch {epoch}/{num_epochs}\t Loss {epoch_loss}')

# train(model, criterion, optimizer, train_ids, train_ids_tag, batch_size, num_epochs)
# torch.save(model.state_dict(), 'model_pos_tagger.pth')

# model.load_state_dict(torch.load("/content/drive/MyDrive/model_pos_tagger.pth",map_location=torch.device('cpu')))
model.load_state_dict(torch.load("model_pos_tagger.pth",map_location=torch.device('cpu')))

from sklearn.metrics import precision_recall_fscore_support,f1_score,accuracy_score

def categorical_accuracy(preds, y, tag_pad_idx=0, average='weighted'):
    """
    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8
    """
    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability
    non_pad_elements = (y != tag_pad_idx).nonzero()
    y_pred = max_preds[non_pad_elements].squeeze(1)
    y_test = y[non_pad_elements]
    y_pred = y_pred.cpu().detach().numpy()
    y_test = y_test.cpu().detach().numpy()
    acc_score = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average=average, labels = np.unique(y_pred))
    pre_rec = precision_recall_fscore_support(y_test,y_pred,average=average, labels=np.unique(y_pred))
    # correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])
    # return correct.sum() / y[non_pad_elements].shape[0]
    return acc_score, f1, pre_rec

def evaluate(model, input_data_sen, input_data_tag, criterion, tag_pad_idx):    
    model.eval()
    epoch_loss = 0
    epoch_acc = 0
    epoch_f1 = 0
    epoch_prec = 0
    epoch_recall = 0
    
    with torch.no_grad():
    
        for ind, batch in enumerate(input_data_sen):
          inputs = input_data_sen[ind].to(device)
          targets = input_data_tag[ind].to(device)
            
          predictions = model(inputs)
          
          predictions = predictions.view(-1, predictions.shape[-1])
          targets = targets.view(-1)
          
          loss = criterion(predictions, targets)
          acc, f1, pre_rec = categorical_accuracy(predictions, targets, tag_pad_idx)
          epoch_loss += loss.item()
          epoch_acc += acc.item()
          epoch_f1 += f1.item()
          epoch_prec += pre_rec[0].item()
          epoch_recall += pre_rec[1].item()
        
    return epoch_acc/len(input_data_sen), epoch_f1/len(input_data_sen), epoch_prec/len(input_data_sen), epoch_recall/len(input_data_sen)

x, y, z, w = evaluate(model, dev_ids, dev_ids_tag, criterion, 0)

print('dev set')
print(f'Accuracy Score: {x}\nF1 Score: {y}\nPrecision Score: {z}\nRecall: {w}')

x, y, z, w = evaluate(model, test_ids, test_ids_tag, criterion, 0)

print('test set')
print(f'Accuracy Score: {x}\nF1 Score: {y}\nPrecision Score: {z}\nRecall: {w}')

sentence = input("input sentence: ")

sentence

import regex as re
def indexer(text, word_to_idx, tag_to_index):
  ids = list()
  for word in text:
    if word in word_to_idx.keys():
      ids.append(word_to_idx[word])
    else:
      ids.append(word_to_idx['<UNK>'])
  ids=torch.from_numpy(np.array(ids))
  return ids

def tagger(text, word_to_idx, tag_to_index):
  text = text.lower()
  text = re.sub(r'[^\w+\s]', r'', text)
  text = re.split(r' ', text)
  ids = indexer(text, word_to_idx, tag_to_index)
  ids = ids.to(device)
  y_pred = model(ids)
  y_pred_main = torch.argmax(y_pred, dim=1).cpu().detach().numpy()
  # y_pred_main = y_pred.argmax(-1)[0].cpu().detach().numpy()
  tags = []

  inv_dict = {v:k for k,v in tag_to_index.items()}
  for i in y_pred_main:
    tags.append(inv_dict[i])
  return text, tags


text, tags = tagger(sentence, word_to_idx, tag_to_index)

for ind, i in enumerate(text):
  print(f'{i}\t{tags[ind]}')

